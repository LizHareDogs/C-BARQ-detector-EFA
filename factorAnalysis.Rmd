---
title: "Factor Analysis of C-BARQ-DD Questionnaire Items"
author: Liz Hare PhD
date: "`r Sys.Date()`"
email: "lizhare@gmail.com"
output:
    html_document:
        toc: true
        toc_depth: 5
        number_sections: true
bibliography: "factorAnalysis.bib"
---

``` {r, setup, results=FALSE, echo=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(gtsummary)
library(psych)
library(mice)
library(mifa)
library(EFAtools)

intFiles <- "/Users/hare/data/dhs/intFiles/"
load(paste0(intFiles, "ddCBARQ.Rda"))
```

# Data Preparation

## Correlation Matrix of Item Responses

The correlation matrix for all pairs of items with pairwise missing values deleted
is in the file `corMatrix1.csv`. It's recommended that some of the items should be correlated
for factor analysis, but that the correlations shouldn't be in the high .9s because that would 
indivcate that the items are too similar. The following table lists the maximum correlation for
each item.

``` {r, corMatrix1, results="asis", echo=FALSE}
corMatrix1 <- round(cor(ddItems, use = "pairwise.complete.obs"), 2)
write.csv(corMatrix1, file=paste0(intFiles, "corMatrix1.csv"))

### As a quick way to get an overview, get max correlation for each colum
### set diagonal to NA since that's the correlation with itself and is always 1
diag(corMatrix1) <- NA
corDF1<- as.data.frame(corMatrix1)
maxCor1 <- sapply(corDF1, max, na.rm=TRUE)
          
### display max correlation for each item                  max)
maxCor1K <- kable(maxCor1,
                  caption = "Maximum Correlation for each Item with All Other Items")
kable_styling(maxCor1K)
```


## Tests for Factorability of Raw Data

### Bartlett Test of Sphericity

From the EFAtools documentation

> This function tests whether a correlation matrix is significantly different from an identity matrix (Bartlett, 1951). If the Bartlett's test is not significant, the correlation matrix is not suitable for factor analysis because the variables show too little covariance.

``` {r, preImputationBartlett, results=FALSE, echo=FALSE}
preBart <- BARTLETT(cor(ddItems, use="pairwise.complete.obs"), N = 734)

```

For the Bartlett test, the chi-square is `r round(preBart$chisq, 2)` with `r round(preBart$df, 2)` DF
and a p-value of `r preBart$p_value`. This significant
value indicates the matrix is different from one with no
relationships between items.

### Kaiser-Meyer-Olkin Criterion

From the EFAtools documentation:

> This function computes the Kaiser-Meyer-Olkin (KMO) criterion overall and for each variable in a correlation matrix. The KMO represents the degree to which each observed variable is predicted by the other variables in the dataset and with this indicates the suitability for factor analysis.

``` {r, preKMO, results=FALSE, echo=FALSE}
preKMO <- KMO(cor(ddItems, use = "pairwise.complete.obs"))
```

The overall KMO is `r round(preKMO$KMO, 2)`, indicating that the data is suitable for factor analysis.



## Imputation for Missing Item Values



### Patterns of Missing data

It's a good idea to check whether the missing values appear in patterns. There are some 
tests to check this but they apply to data sets with < 50 variables (https://statistics.ohlsen-web.de/multiple-imputation-with-mice/).  

The plot below should display patterns of missing responses. Let me know if anything is interesting. 

``` {r, missPattern, results="asis", echo=FALSE}
missPattern <- md.pattern(ddItems)

```


Currently recommendations for handling missing values are that some methods
used in the past, like pairwise deletion and substituting the mean or median value for the item,
are biased and should not be used. The suggestion of removing individuals with any missing
values isn't practical for this study; there are reasons particular items might not apply 
to particular dogs.  

Multiple imputation can be used to produce a 
covariance matrix that can be input into factor analysis (this matrix is 
created as part of the factor analysis, which accepts 
either individual records or covariances. the `mifa` R package [@nassiri18]
produces a series of covariate matrices by 
and The matrices are then combined by to produce a final estimate.  

The default number of matrices to impute is 5. I did 100 to make sure we were doing enough. Their recommendations were lower.



``` {r, imputation, results=FALSE, echo=FALSE}
load(paste0(intFiles, "imputedFactors.Rda"))
### already run and saved:
### imputed <- mifa(ddItems, m=100)
### save(imputed, file=paste0(intFiles, "imputedFactors.Rda"))
```

``` {r, imputationFactors, results="asis", echo=FALSE}
### plot and table for number of factors and variables explained (imputed$var_explained
varExplainedPlot <- ggplot(imputed$var_explained, aes(x = n_pc, y= var_explained)) +
                                        geom_line() +
                                        labs(x = "Number of Factors", y = "Variance Explained",
                                             title="Variance Explained for Each Possible Number of Factors")
varExplainedPlot

varExplainedK <- kable(imputed$var_explained, digits=2,
                       caption="Percent of Variance Explained for Each Potential Number of Factors")
kable_styling(varExplainedK)
```

It seems a little strange that there isn't any error variance? Oh, PCA is the one 
with the error variance, but that is more focused on individual differences than 
whether there are shared underlying factors.


### Tests of Factorability for Imputed data
#### Bartlett's Test of Sphericity

From the `EFAtools` documentation:  

> This function tests whether a correlation matrix is significantly different from an identity matrix (Bartlett, 1951). If the Bartlett's test is not significant, the correlation matrix is not suitable for factor analysis because the variables show too little covariance.  

``` {r, bartlett, results=FALSE, echo=FALSE}
### some of these require correlation rather than cov matrix as input
imputedCorr <- cov2cor(imputed$cov_combined)
bart <- BARTLETT(imputedCorr, N=734)
```

For the Bartlett test chi-square = `r round(bart$chisq, 2)` with 
`r bart$df` DF and p = 
`r bart$p_value`. A significant p value indicates
that the data are suitable for factoring.

#### Kaiser-Meyer-Olkin Criterion

From the EFAtools documentation:

This function computes the Kaiser-Meyer-Olkin (KMO) criterion overall and for each variable in a correlation matrix. The KMO represents the degree to which each observed variable is predicted by the other variables in the dataset and with this indicates the suitability for factor analysis.

``` {r, KMO, results=FALSE, echo=FALSE}
kmoc <- KMO(imputedCorr)
```


The overall KMO value for the imputed data set is `r round(kmoc$KMO, 2)`,
indicating the data are probably suitable for factor analysis. 
KMO values for individual items are also available.

# Factor Analysis

## Tools for Estimating Number of Factors

There are many available methods for estimating the number of factors to extract. 
Luckily, they suggest using multiple ways that are compatible with the data, along
with applying knowledge of the subject matter to examine which factors make sense. I think
the Kaiser Guttman Criterion was used for the original C-BARQ, but it has since received a lot of
criticism.  

There are recommendations from 3 to 33 factors, so we have some exploring to do.

### Empirical Kaiser Criterion

From EFAtools package documentation:

> The empirical Kaiser criterion incorporates random sampling variations of the eigenvalues from the Kaiser-Guttman criterion (KGC; see Auerswald & Moshagen , 2019; Braeken & van Assen, 2017). The code is based on Auerswald and Moshagen (2019).

``` {r, empiricalKaiserCriterion, results=FALSE, echo=FALSE}
ekcImputed <- EKC(imputedCorr, N = 734)
```

This method recommends `r ekcImputed$n_factors` factors for this data.

### Hull method

From EFAtools documentation:

> The Hull method aims to find a model with an optimal balance between model fit and number of parameters. That is, it aims to retrieve only major factors (Lorenzo-Seva, Timmerman, & Kiers, 2011  

Settings used:
- method = ULS for least-squares estimation, similar to original C-BARQ analysis, robust to non-normal items- eigen_type = EFA for exploratory factor analysis rather than other types
- 
``` {r, hullMethods, results="asis", echo=FALSE}
hullImputed <- HULL(imputedCorr, N = 734, method="ULS", eigen_type = "EFA")
hullImputed
```

Abbreviations for Goodness-of-Fit Indices (from Lorenzo-Seva et  al, 2011)

- CAF - 
Common Part Accounted FOr (derived in Lorenzo-Seva et al, 2001)

> Expresseses the extent to which the common variance in the data is captured in the common factor model. Range 0 - 1 where close to 0 a substantial amond ot common variance is still present in the residutal matrix after the factors (implying that more factors should be extraced.. Close to 1 means the residual matrix is free of common variance after the factors have been extracted.  

- CFI - Comparative Fit Index

> measures improvement in fit by comparing hypothesized model with a more restrictive,
baseline model model. This model is usually a null model with no correlation
observed variables. Range is from 0 to 1, where 1 is perfect fit.  

- RMSEA - Root Mean Square Errorof Approximation

> basedon analysis of residuaals and reflects the 
degree of misfit in proposed model.ranges from 0 to infinity 
with 0 being perfect fit and larger numbers indicating poorer fit.

### Kaiser-Guttman Criterion

I think this was used for the original C-BARQ.

From the EFAtools documentation:

>Probably the most popular factor retention criterion. Kaiser and Guttman suggested to retain as many factors as there are sample eigenvalues greater than 1. This is why the criterion is also known as eigenvalues-greater-than-one rule.


Settings Used:
- eigen__type = EFA


``` {r, kaiserGutman, results="asis", echo=FALSE}
KGCimputed <- KGC(imputedCorr, eigen_type = "EFA")
```

this method suggests extracting `r KGCimputed$n_fac_EFA` factors.

### Parallel Method

From the EFAtools documentation:

>Various methods for performing parallel analysis. This function uses future_lapply for which a parallel processing plan can be selected. To do so, call library(future) and, for example, plan(multisession); see examples.  

Settings Used:  
- N = 734
- eigen_type = "EFA"

``` {r, parallelFactorestimation, results=FALSE, echo=FALSE}
parallelImputed <- PARALLEL(imputedCorr, N = 734, eigen_type = "EFA")
```

This method suggests using `r parallelImputed$n_fac_EFA` factors.

#### Comparison Data

This is a variant of the parallel method. From the EFAtools documentation:

>"Parallel analysis (PA) is an effective stopping rule that compares the eigenvalues of randomly generated data with those for the actual data. PA takes into account sampling error, and at present it is widely considered the best available method. We introduce a variant of PA that goes even further by reproducing the observed correlation matrix rather than generating random data. Comparison data (CD) with known factorial structure are first generated using 1 factor, and then the number of factors is increased until the reproduction of the observed eigenvalues fails to improve significantly" (Ruscio & Roche, 2012, p. 282).  

* Oops, this doesn't work when there is missing raw data! Tested with small fake data set. 
(Our imputed data is the correlation/covariance matrix, not replacements for missing raw data. 
Goretzko, 2021) recommends using a random forest imputation method instead of mice.

``` {r, comparisonDataMethod, results="asis", echo=FALSE}
### cdImputed <- CD(ddItems, use="pairwise.complete.obs", N_pop = 734)
```


### Sequential Chi-Square Model Tests

From the EFAtools documentation:

> Sequential Chi Square Model Tests (SMT) are a factor retention method where multiple EFAs with increasing numbers of factors are fitted and the number of factors for which the Chi Square value first becomes non-significant is taken as the suggested number of factors. Preacher, Zhang, Kim, & Mels (2013) suggested a similar approach with the lower bound of the 90% confidence interval of the Root Mean Square Error of Approximation (RMSEA; Browne & Cudeck, 1992; Steiger & Lind, 1980), and with the Akaike Information Criterion (AIC). For the RMSEA, the number of factors for which this lower bound first falls below .05 is the suggested number of factors to retain. For the AIC, it is the number of factors where the AIC is lowest.



``` {r, sequentialModelTests, results="asis", echo=FALSE}
SMTimputed <- SMT(imputedCorr, N = 734)
SMTimputed
```

### Scree Plot

From the EFAtools documentation:

> The scree plot was originally introduced by Cattell (1966) to perform the scree test. In a scree plot, the eigenvalues of the factors / components are plotted against the index of the factors / components, ordered from 1 to N factors components, hence from largest to smallest eigenvalue. According to the scree test, the number of factors / components to retain is the number of factors / components to the left of the "elbow" (where the curve starts to level off) in the scree plot.



``` {r, screePlot, results="asis", echo=FALSE}
SCREE(imputedCorr, eigen_type = "EFA")
```
  

## Unweighted Least Squares, Varimax Rotation, 

This is the method used for the original C-BARQ. Using least squares is good when you have 
non-normality. Varimax rotation might not be the best chose because it doesn't allow
covariance between factors, which we probably have (like stranger, owner, and dog-directed 
aggression). But we should replicate the original analysis so we can compare findings.  

I have run this analysis for the possibilities of anywhere from 5 15 factors, but am still sorting through the large output produced to figure out how to present it in a way that's not overwhelming.

``` {r, EFAvarimax, results="asis", echo=FALSE}
### run a series with n_factors 5-20



## EFA(imputedCorr, n_factors = c(5:15), N = 734, method="ULS", rotation="varimax")
## prefix <- "Umodel"
## suffix <- c(5:20)
## UmodelNames <- paste0(prefix, suffix)
## for (i in suffix) {
##     UmodelNames[i] <- EFA(imputedCorr,
##     n_factors = suffix[i],
##     N = 734,
##     method = "ULS",
##     rotation = "varimax")
## }




## numFact <- c(5:20)
## ULSvarimax <- sapply(numFact, function(x) 
##     EFA(imputedCorr,
##         n_factors = x,
##         N = 734,
##         method = "ULS",
##         rotation = "varimax")
## )
## )for (x in 5:20) {

    
##     EFA_ULS_varimax$x <- EFA(imputedCorr,
##                        n_factors = x,
##                        N = 734,
##                        method = "ULS",
##                        rotation = "varimax")
## }
## ### the type option is not needed for ULS

## ### extract indices of fit

```
